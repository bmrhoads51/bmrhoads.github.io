<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Project 1</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
        body { 
            font-family: Arial, sans-serif; 
            margin: 20px; 
            /* Center the container horizontally */
            display: flex; 
            justify-content: center;
        }
        .container {
            /* Constrain max width but allow to fill smaller windows */
            max-width: 900px;
            width: 100%;
        }
        .section { margin-bottom: 40px; }
        .images-row {
            display: flex;
            gap: 20px;
            margin-top: 10px;
            margin-bottom: 10px;
        }
        .images-row figure {
            flex: 1;
            margin: 0;
            display: flex;
            flex-direction: column;
            align-items: center;
        }
        .images-row figure img {
            width: 70%;
            height: auto;
            border-radius: 6px;
            display: block;
        }
        .images-row figcaption {
            width: 70%;
            text-align: center;
            font-size: 0.95em;
            color: #555;
            margin-top: 5px;
        }
        .images-row .less-width {
            flex: 0.5;
        }
        .images-row .more-width {
            flex: 1.5;
        }
        .images-row.full-width-image-row {
            /* Remove any gap/padding if you don't want space between container and image */
            gap: 0;
        }
        
        .images-row.full-width-image-row figure {
            flex: 1 1 100%;
            margin: 0;
            width: 100%;
            display: flex;
            flex-direction: column;
            align-items: center; /* optional if you want to center caption */
        }
        
        .images-row.full-width-image-row figure img {
            width: 100%;
            height: auto;
            display: block;
            border-radius: 0; /* Remove rounding if you want image edge-to-edge */
        }
        .images-row.full-width-image-row figure figcaption {
            width: 100%;
        }
        h1, h2, h3, h4 { margin-top: 0; }
    </style>
</head>
<body>
    <div class="container">
    <a href="index.html">Go Back</a>
    <p></p>

    <h1>Project 1</h1>

    <div class="section">
        <h2>Part 1 - Perspective and orthographic projections</h2>
        <p>I took a picture of a bench on campus from up close (bottom left image) and one from far away (bottom right image), and then cropped the right photo so the bench was a similar size as the one from up close. I then drew red lines on the parallel edges in both images to see whether the images were orthogonal or perspective.</p>
        <div class="images-row">
            <figure>
                <img src="project-1-images/close_up_bench-marked.jpg" alt="Bench picture from close up">
                <figcaption>Picture of bench taken from up close with converging red lines drawn on parallel edges showing perspective.</figcaption>
            </figure>
            <figure>
                <img src="project-1-images/zoomed_out_bench-marked.jpg" alt="Bench picture from far away">
                <figcaption>Picture of bench taken from far away, cropped to a similar size as close up image. Red lines show parallel lines stay parallel, meaning image is orthogonal.</figcaption>
            </figure>
        </div>
        <p>The left photo clearly shows that the lines are not parallel, meaning this is a perspective photo. The right photo shows that the lines are still parallel, meaning this is orthogonal. The orthogonal picture does not show perspective because the photo was taken from far away, and therefore there is relatively less difference in depth from the front of the bench to the end, so the effect of depth cannot be shown, hence it is orthogonal.</p>
    </div>

    <div class="section">
        <h2>Part 2 - Histogram Manipulation & Linear Filtering</h2>
        <h3>Question (a) - Histogram Equalization on Provided Images </h3>
        <p>In the first image, the image is too bright, meaning the histogram is shifted right, and the CDF is flat on the left side. By taking the minimum and maximum pixel values, they can be normalized between 0 and 255 so the cdf begins at 0 and increases until it reaches 255.</p>
        <div class="images-row">
            <figure>
                <img src="project-1-images/beans_1_grid.png" alt="beans_1_grid">
                <figcaption>Original image (1), histogram, and CDF in top row, and equalized image, histogram, and CDF in bottom row.</figcaption>
            </figure>
        </div>
        <p>In the second picture, the image does not have dark or light pixels, meaning the histogram needs to be stretched in both directions</p>
        <div class="images-row">
            <figure>
                <img src="project-1-images/beans_2_grid.png" alt="beans_2_grid">
                <figcaption>Original image (2), histogram, and CDF in top row, and equalized image, histogram, and CDF in bottom row.</figcaption>
            </figure>
        </div>
        <p>In the last image, pixels are all too dark, so the histogram needs to be stretched to include pixels with higher values.</p>
        <div class="images-row">
            <figure>
                <img src="project-1-images/beans_3_grid.png" alt="beans_3_grid">
                <figcaption>Original image (3), histogram, and CDF in top row, and equalized image, histogram, and CDF in bottom row.</figcaption>
            </figure>
        </div>
    
        <h3>Question (b) - Creative Task: Apply to Your Own Images</h3>
        <p>The winter picture shown below does not have enough contrast. Additionally, while the histogram looks like most pixels are between 100 and 200, they in fact get as low as 23 and as high as 224, meaning the histogram is very skewed on both sides. This is why equalization, shown in the second row, does not have a significant impact on the histogram and the overal contrast. To further improve the contrast, I multiplied the pixel values by 1.8 and then subtracted 160. Pixel value above 255 or below 0 were clipped at 255 and 0 respectively, and since there are very few pixels at these extreme values, it does not significantly alter the overall image. I adjusted these values until the histogram visibly looked better, and the CDF increased from pixel values of 0 to 255. In the last row, it is clear that the contrast is significantly improved, showing that equalization alone is not always successful.</p>
        <p>It is also worth noting that equalization may not always be necessary, since the winter image below may have been intended to have low contrast to have a certain tone (cloudy during the winter time), while the equalization makes the scene look a little less cloudy. This means the intended use of the picture must be taken into account before processing an image. </p>
        <div class="images-row">
            <figure>
                <img src="project-1-images/winter_grid.png" alt="winter_grid">
                <figcaption>Original image (top row), equalized image (middle row), and equalized, scaled, shifted, and clipped (bottom row).</figcaption>
            </figure>
        </div>
    
        <h3>Question (c) - Histogram Matching</h3>
        <p>For the three images, matching each of them to the target image is similar to equalization because the target image has complete contrast. The following three figure show the histogram matching of the three images. </p>
        <div class="images-row">
            <figure>
                <img src="project-1-images/beans_1_target.png" alt="beans_1_target">
                <figcaption>Original image, histogram, and CDF in top row, and equalized image, histogram, and CDF in bottom row.</figcaption>
            </figure>
        </div>
        <div class="images-row">
            <figure>
                <img src="project-1-images/beans_2_target.png" alt="beans_2_target">
                <figcaption>Original image, histogram, and CDF in top row, and equalized image, histogram, and CDF in bottom row.</figcaption>
            </figure>
        </div>
        <div class="images-row">
            <figure>
                <img src="project-1-images/beans_3_target.png" alt="beans_3_target">
                <figcaption>Original image, histogram, and CDF in top row, and equalized image, histogram, and CDF in bottom row.</figcaption>
            </figure>
        </div>
    
        <h3>Question (d)</h3>
        <p>Using a Gaussian filter, [[1,2,1], [2,4,2], [1,2,1]], followed by a simple derivative filter in the x [-1,0,1] and y [[-1], [0], [1]] direction, the edges of the x and y components were obtained and used to calculate the magnitude of the edge vector. Then, the derivative of the gaussian filter in the x and y direction was applied to the images to directly detect the x and y components of the edges, which were also combined for the complete edge magnitude. The figures below show the naive (two step) and direct (single step) x, y, and magnitude images of the edges for the 3 test images. </p>
        
        <div class="images-row full-width-image-row">
            <figure class="less-width">
                <img src="project-1-images/image_4.png" alt="image_4">
                <figcaption>Original image</figcaption>
            </figure>
            <figure class="more-width">
                <img src="project-1-images/image_4_edge.png" alt="image_4">
                <figcaption>dx, dy, and edges using naive and direct gaussian filter approach. </figcaption>
            </figure>
        </div>
        <div class="images-row full-width-image-row">
            <figure class="less-width">
                <img src="project-1-images/image_5.png" alt="image_5">
                <figcaption>Original image</figcaption>
            </figure>
            <figure class="more-width">
                <img src="project-1-images/image_5_edge.png" alt="image_5">
                <figcaption>dx, dy, and edges using naive and direct gaussian filter approach. </figcaption>
            </figure>
        </div>
        <div class="images-row full-width-image-row">
            <figure class="less-width">
                <img src="project-1-images/image_6.png" alt="image_6">
                <figcaption>Original image</figcaption>
            </figure>
            <figure class="more-width">
                <img src="project-1-images/image_6_edge.png" alt="image_6">
                <figcaption>dx, dy, and edges using naive and direct gaussian filter approach. </figcaption>
            </figure>
        </div>
        <p>In the first image (circle inside of square), there is consistent noise which is not fully removed from the gaussian smoothing, so the final edge pictures also have some noise, but the edges are still clearly defined. This is because the image is overall clear in the separation between sections. </p>
        <p>In the second image (circuit board), the naive and direct methods both do nearly the same; however, the naive method appears to have more crisp edges. This may be because using two filters can preserve more information than using just one, since they are both approximations of gaussian filters. It is also likely that the DOG filters being a 3x3 matrix causes additional surrounding pixels to be included in the derivative calculation, causing more blurry edges, while the direct approach just uses row and column vectors for the derivatives, reducing the noise in the edge calculation. </p>
        <p>Finally, the last image, which is the same as the previous one, but with salt and pepper noise. It is known that salt and pepper noise is difficult to remove with gaussian filters, especially when they are such small filters, since a white or black pixel can significantly affect the value of the smoothed pixel and the local derivative. The naive method also appears to have extracted more crisp edges in this case as well, likely for the same reasons as described for the previous image. </p>
        
        <h3>Question (e) - Image Sharpening</h3>
        <p>For the images below, each one was smoothed using an approximate Gaussian filter and then the smoothed image was subtracted from the original image to obtain the high-frequency component. This high-frequency component was added to the original image in order to sharpen the image and magnify any edges that were unclear. The different alpha values used, which is multiplied by the high-frequency components, were 1, 3, 5, and 10. Increasing the alpha value generally increases the clarity/magnitude of the edges. The first two rows, the CT scan and the moon, do not show the edges very well, so the sharpening is intended to magnify the edges. For the CT scan, there is a small improvement, but it is difficult to improve this due to the noise throughout the image. In the moon image, there is not too much noise - just low resolution. Therefore, the sharpening significantly improved the edges on the image and the craters are much more visible.</p>
        <p>The next row is an image of a wolverine taken by a tourist, but the image is blurry and is not very clear. In this case, the edges can be sharpened using this sharpening technique, and the results are very good. An alpha of about 3 or 5 looks like a much more realistic image, with much clearer edges, while the alpha of 10 seems to overdo the sharpening, adding clear edges where there likely should not be.</p>
        <p>The final row shows an example where this sharpening method is not successful. In this image, there is some pixelation, similar to salt and pepper noise in that it is very pixelated noise. During the sharpening, the pixelated parts of the image are considered to have their own edges, so they remain in the sharpened image. This is an example showing that salt and pepper noise or pixelated (low resolution) edges cannot be enhanced easily with sharpening techniques. </p>
        
        <div class="images-row full-width-image-row">
            <figure>
                <img src="project-1-images/sharpening.png" alt="sharpening multiple images">
                <figcaption></figcaption>
            </figure>
        </div>
    </div>

    <div class="section">
        <h2>Part 3 - Anisotropic Diffusion</h2>
        <h3>Question 1 - Implementation of Anisotropic Diffusion</h3>
        <p>For the image of the woman, the first option is able to remove most of the noise with a K of 30, although some of the detail is lost. For the second image, a lot of noise is removed with K=20, and even more with K=30. For K values of 20 and 30, it looks like too much detail is lost by removing noise, and for 30, there is even some significant blurring happening. K=10 is a good reduction in noise without significantly losing information about the CT scan.</p>
        <p>In the bottom two rows, using option 2 for the diffusion coefficient, the output images are significantly different. They become much darker than option 1 images, almost as if surrounding pixels are being combined to form a darker pixel. In the image of the woman, this makes it look like the lighting is changing, and there are just more shadows around her face, but in the CT scan, there is not as much darkening. There is still a significant amount of blurring, like with option 1, on the CT scan, and a K of 10 is probably too much blurring in this case. </p>
        <div class="images-row full-width-image-row">
            <figure>
                <img src="project-1-images/anisotropic_diffusion.png" alt="anisotropic diffusion">
                <figcaption>Applying anisotropic diffusion on two images with increasing K values. Top two rows are for diffusion coefficent calculated using exponential term (option 1) and bottom two rows are using diffusion coefficient with fractional term (option 2). </figcaption>
            </figure>
        </div>
        <p>Overall, the anisotropic diffusion filter is able to remove noise very well, but if the filter is too strong (K value in diffusion coefficient calculation), there can be a lot of blurring, making the image worse. Tuning this parameter for the specific task is necessary, as with most image processing methods. </p>
        
        <h3>Question 2 - Comparison with Gaussian Smoothing</h3>
        <p>The anisotropic diffusion using the exponential diffusion coefficient is much better at preserving the lighting contrast than the gaussian filter. It also appears to preserve most of the details in the image, including the woman's hair. The amount of detail in the hair appears is almost indistinguishable from that of the original image. </p>
        <p>The gaussian smoothening seems to affect the lighting contrast similar to the anisotropic diffusion with the fractional diffusion coefficient, while not removing as much noise. The anisotropic diffusion image seems to have more clear edges, and about the same amount of detail. Reducing the K value may help with some of these problems, as shown in the previous image. </p>
        <p>The results are very similar for the CT scan, however the lighting is not affected as much as the image of the woman, and the gaussian filter does not remove as much noise as either of the anisotropic diffusion filters. </p>
        <p>Additionally, increasing the number of iterations appears to have a similar affect as increasing the value of K. A K value of 30 with 15 iterations is similar to using a K value of 15 with 30 iterations, as both parameters are affecting the degree to which the smoothening is being applied. </p>
        <div class="images-row full-width-image-row">
            <figure>
                <img src="project-1-images/anisotropic_vs_gaussian.png" alt="anisotropic diffusion vs gaussian comparison">
                <figcaption>Comparison of gaussian smoothening (second column) to anisotropic diffusion with K value of 20 (third column: diffusion coefficient with exponential; fourth column: diffusion coefficient with fraction). . </figcaption>
            </figure>
        </div>
        <p>Overall, using the exponential diffusion coefficient equation seems to preserve contrast much better than the fraction diffusion coefficient and gaussian smoothening, and it also preserves details much better.</p>
        
        <h3>Question 3 - Limitations of Anisotropic Diffusion and Potential Improvements</h3>
        <p>One limitation of the anisotropic diffusion filter is the amount of iterations required, which makes it the number of iterations more complex than a gaussian filter, not including the higher complexity in the calcultion of a single iteration compared to the gaussian filter. Additionally, the K value and iterations must both be tuned to optimize smoothening (cannot smooth too little, and definitely don't want to smooth too much). It may be possible to reduct the iterations needed to a much smaller number (perhaps even 1) if the K value is high enough. This would require only optimizing one parameter. Another possibility for improvement would be to include diagonal pixels in the local gradient calculation, which would provide a more complete calculation of the local landscape for smoothening. This, however, would require twice the computation (8 gradients instead of 4). Last, maybe it would improve the smoothening if the anisotropic diffusion and gaussian smoothening were used serially. Alternating between the two filters (difficult to say what order would be best; it is possible order would not matter because of commutative property of convolutions), may allow the resulting image to benefit from the strengths of each method, improving the overall smoothed image. </p>
    </div>
    </div>
</body>
</html>
