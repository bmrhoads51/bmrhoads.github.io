<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Project 1</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
        body { 
            font-family: Arial, sans-serif; 
            margin: 20px; 
            /* Center the container horizontally */
            display: flex; 
            justify-content: center;
        }
        .container {
            /* Constrain max width but allow to fill smaller windows */
            max-width: 900px;
            width: 100%;
        }
        .section { margin-bottom: 40px; }
        .images-row {
            display: flex;
            gap: 20px;
            margin-top: 10px;
            margin-bottom: 10px;
        }
        .images-row figure {
            flex: 1;
            margin: 0;
            display: flex;
            flex-direction: column;
            align-items: center;
        }
        .images-row figure img {
            width: 70%;
            height: auto;
            border-radius: 6px;
            display: block;
        }
        .images-row figcaption {
            width: 70%;
            text-align: center;
            font-size: 0.95em;
            color: #555;
            margin-top: 5px;
        }
        .images-row .less-width {
            flex: 0.5;
        }
        .images-row .more-width {
            flex: 1.5;
        }
        h1, h2, h3, h4 { margin-top: 0; }
    </style>
</head>
<body>
    <div class="container">
    <a href="index.html">Go Back</a>


    <h1>Project 1</h1>

    <div class="section">
        <h2>Part 1 - Perspective and orthographic projections</h2>
        <p>I took a picture of a bench on campus from up close (bottom left image) and one from far away (bottom right image), and then cropped the right photo so the bench was a similar size as the one from up close. I then drew red lines on the parallel edges in both images to see whether the images were othogonal or perspective.</p>
        <div class="images-row">
            <figure>
                <img src="project-1-images/close_up_bench-marked.jpg" alt="Bench picture from close up">
                <figcaption>Picture of bench taken from up close with converging red lines drawn on parallel edges showing perspective.</figcaption>
            </figure>
            <figure>
                <img src="project-1-images/zoomed_out_bench-marked.jpg" alt="Bench picture from far away">
                <figcaption>Picture of bench taken from far away, cropped to a similar size as close up image. Red lines show parallel lines stay parallel, meaning image is orthogonal.</figcaption>
            </figure>
        </div>
        <p>The left photo clearly shows that the lines are not parallel, meaning this is a perspective photo. The right photo shows that the lines are still parallel, meaning this is orthogonal. The orthogonal picture does not show perspective because the photo was taken from far away, and therefore there is relatively less difference in depth from the front of the bench to the end, so the effect of depth cannot be shown, hence it is orthogonal.</p>
    </div>

    <div class="section">
        <h2>Part 2 - Histogram Manipulation & Linear Filtering</h2>
        <h3>Question (a) - Histogram Equalization on Provided Images </h3>
        <p>In the first image, the image is too bright, meaning the histogram is shifted right, and the CDF is flat on the left side. By taking the minimum and maximum pixel values, they can be normalized between 0 and 255 so the cdf begins at 0 and increases until it reaches 255.</p>
        <div class="images-row">
            <figure>
                <img src="project-1-images/beans_1_grid.png" alt="beans_1_grid">
                <figcaption>Original image (1), histogram, and CDF in top row, and equalized image, histogram, and CDF in bottom row.</figcaption>
            </figure>
        </div>
        <p>In the second picture, the image does not have dark or light pixels, meaning the histogram needs to be stretched in both directions</p>
        <div class="images-row">
            <figure>
                <img src="project-1-images/beans_2_grid.png" alt="beans_2_grid">
                <figcaption>Original image (2), histogram, and CDF in top row, and equalized image, histogram, and CDF in bottom row.</figcaption>
            </figure>
        </div>
        <p>In the last image, pixels are all too dark, so the histogram needs to be stretched to include pixels with higher values.</p>
        <div class="images-row">
            <figure>
                <img src="project-1-images/beans_3_grid.png" alt="beans_3_grid">
                <figcaption>Original image (3), histogram, and CDF in top row, and equalized image, histogram, and CDF in bottom row.</figcaption>
            </figure>
        </div>
    
        <h3>Question (b) - Creative Task: Apply to Your Own Images</h3>
        <p>The winter picture shown below does not have enough contrast. Additionally, while the histogram looks like most pixels are between 100 and 200, they in fact get as low as 23 and as high as 224, meaning the histogram is very skewed on both sides. This is why equalization, shown in the second row, does not have a significant impact on the histogram and the overal contrast. To further improve the contrast, I multiplied the pixel values by 1.8 and then subtracted 160. Pixel value above 255 or below 0 were clipped at 255 and 0 respectively, and since there are very few pixels at these extreme values, it does not significantly alter the overall image. I adjusted these values until the histogram visibly looked better, and the CDF increased from pixel values of 0 to 255. In the last row, it is clear that the contrast is significantly improved, showing that equalization alone is not always successful.</p>
        <p>It is also worth noting that equalization may not always be necessary, since the winter image below may have been intended to have low contrast to have a certain tone (cloudy during the winter time), while the equalization makes the scene look a little less cloudy. This means the intended use of the picture must be taken into account before processing an image. </p>
        <div class="images-row">
            <figure>
                <img src="project-1-images/winter_grid.png" alt="winter_grid">
                <figcaption>Original image (top row), equalized image (middle row), and equalized, scaled, shifted, and clipped (bottom row).</figcaption>
            </figure>
        </div>
    
        <h3>Question (c) - Histogram Matching</h3>
        <p>For the three images, matching each of them to the target image is similar to equalization because the target image has complete contrast. The following three figure show the histogram matching of the three images. </p>
        <div class="images-row">
            <figure>
                <img src="project-1-images/beans_1_target.png" alt="beans_1_target">
                <figcaption>Original image, histogram, and CDF in top row, and equalized image, histogram, and CDF in bottom row.</figcaption>
            </figure>
        </div>
        <div class="images-row">
            <figure>
                <img src="project-1-images/beans_2_target.png" alt="beans_2_target">
                <figcaption>Original image, histogram, and CDF in top row, and equalized image, histogram, and CDF in bottom row.</figcaption>
            </figure>
        </div>
        <div class="images-row">
            <figure>
                <img src="project-1-images/beans_3_target.png" alt="beans_3_target">
                <figcaption>Original image, histogram, and CDF in top row, and equalized image, histogram, and CDF in bottom row.</figcaption>
            </figure>
        </div>
    
        <h3>Question (d)</h3>
        <p>Using a Gaussian filter, [[1,2,1], [2,4,2], [1,2,1]], followed by a simple derivative filter in the x [-1,0,1] and y [[-1], [0], [1]] direction, the edges of the x and y components were obtained and used to calculate the magnitude of the edge vector. Then, the derivative of the gaussian filter in the x and y direction was applied to the images to directly detect the x and y components of the edges, which were also combined for the complete edge magnitude. The figures below show the naive (two step) and direct (single step) x, y, and magnitude images of the edges for the 3 test images. </p>
        
        <div class="images-row">
            <figure class="less-width">
                <img src="project-1-images/image_4.png" alt="image_4">
                <figcaption>Original image</figcaption>
            </figure>
            <figure class="more-width">
                <img src="project-1-images/image_4_edge.png" alt="image_4">
                <figcaption>dx, dy, and edges using naive and direct gaussian filter approach. </figcaption>
            </figure>
        </div>
        <div class="images-row">
            <figure class="less-width">
                <img src="project-1-images/image_5.png" alt="image_5">
                <figcaption>Original image</figcaption>
            </figure>
            <figure class="more-width">
                <img src="project-1-images/image_5_edge.png" alt="image_5">
                <figcaption>dx, dy, and edges using naive and direct gaussian filter approach. </figcaption>
            </figure>
        </div>
        <div class="images-row">
            <figure class="less-width">
                <img src="project-1-images/image_6.png" alt="image_6">
                <figcaption>Original image</figcaption>
            </figure>
            <figure class="more-width">
                <img src="project-1-images/image_6_edge.png" alt="image_6">
                <figcaption>dx, dy, and edges using naive and direct gaussian filter approach. </figcaption>
            </figure>
        </div>
        <p>In the first image (circle inside of square), there is consistent noise which is not fully removed from the gaussian smoothing, so the final edge pictures also have some noise, but the edges are still clearly define. This is because the image is overall clear in the separation between sections. </p>
        <p>In the second image (circuit board), the naive and direct methods both do nearly the same; however, the naive method appears to have more crisp edges. This may be because using two filters can preserve more information than using just one, since they are both approximations of gaussian filters. It is also likely that the DOG filters being a 3x3 matrix causes additional surrounding pixels to be included in the derivative calculation, causing more blurry edges, while the direct approach just uses row and column vectors for the derivatives, reducing the noise in the edge calculation. </p>
        <p>Finally, the last image, which is the same as the previous one, but with salt and pepper noise. It is known that salt and pepper noise is difficult to remove with gaussian filters, especially when they are such small filters, since a white or black pixel can significantly affect the value of the smoothed pixel and the local derivative. The naive method also appears to have extracted more crisp edges in this case as well, likely for the same reasons as described for the previous image. </p>
        
        <h3>Question (e) - Image Sharpening</h3>
        <p></p>
        <div class="images-row">
            <figure>
                <img src="project-1-images/sharpening.png" alt="Bench picture from close up">
                <figcaption></figcaption>
            </figure>
        </div>
        <p></p>
    </div>

    <div class="section">
        <h2>Part 3 - Anisotropic Diffusion</h2>
        <p>Another section with just text, or you could add more images here.</p>
    </div>
    </div>
</body>
</html>
